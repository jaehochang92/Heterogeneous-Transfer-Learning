# Transfer Learning with missing covariates - sieve
source('sv-prim.R')

library(MASS)
library(glmnet)
library(stargazer)
library(pbapply)
library(tidyr)
library(truncnorm)
library(Sieve)

# https://github.com/terrytianyuzhang/Sieve/blob/master/R/SieveFittingModels.R

# DGP ---------------------------------------------------------------------

# n <- 3e2
# p1 <- p2 <- 7
# A <- replicate(p2, 1:3)
# Î£x <- diag(2, p1)
# Î£Î¾ <- diag(5, p2)
# GenD(n, p1, p2, Î£x, Î£Î¾, A)

GenD <- function(n, p1, p2, Î£x, Î£Î¾, A, alg = 'jae', trgt = F) {
  if (alg == 'jae') {
    if (!is.null(Î£x)) {
      X <- mvrnorm(n, rep(0, p1), Î£x)
    } else {
      X <- matrix(runif(n * p1, -2, 2), n)
    }
    tb <- tibble(X = X)
    if (p2 > 0) {
      Îž <- mvrnorm(n, rep(0, p2), Î£Î¾)
      Z <- H(X, p2, A, trgt) + Îž
      tb$Z <- Z
      pairs(data.frame(X = X, Z = Z), cex = .1, main = n)
      Sys.sleep(1)
    }
  }
  return(tb)
}

### Data-Generating Process
# K: # of proxy studies
# np: sample size of proxy studies (> d, set to be identical across K studies)
# nt: sample size of target study (typically nt{\gg}log(p1 + p2))
# p1, p2: # of matched and mismatched features (may depend on nt)
# sparse: whether beta contrast should be sparse (proximity)
DGP_transfer = function(K, np, nt, p1, p2, ntest, Prxy, Trgt) {
  p <- p1 + p2
  ð’« <- list()
  Î´st <- list()
  E.cum <- mvrnorm(max(np * K, nt + ntest), c(0, 0), matrix(c(1, .5, .5, 1), 2))
  A <- replicate(p2, 1:5)
  # A <- replicate(p2, 1:round(log(p1, 2)))
  # A <- replicate(p2, sample(1:p1, size = round(log(p1, 2))))
  
  ## Proxy domain
  for (k in 1:K) {
    Îµp <- E.cum[(np * (k - 1) + 1):(np * k), 2]
    ## difference of beta1
    Dp <- GenD(np, p1, p2, Prxy$Î£xp[[k]][1:p1, 1:p1], 
               Prxy$Î£Î¾p[[k]][1:p2, 1:p2], A)
    Î´st[['sp']] <- Prxy$Î´st_sprs[[k]][1:p]
    Î´st[['nsp']] <- Prxy$Î´st_nsprs[[k]][1:p]
    for (Î´sti in c('sp', 'nsp')) {
      Î²pk <- Trgt$Î²t[1:p] - Î´st[[Î´sti]][1:p]
      Yp <- as.matrix(Dp) %*% Î²pk + Îµp
      proxydata = tibble(Y = c(Yp), D = Dp)
      ð’«[[Î´sti]][[k]] <- proxydata
    }
  }
  
  ## Target domain
  Îµt     = E.cum[1:nt, 1]
  Îµtest  = E.cum[1:ntest + nt, 1]
  Dtrgt <- GenD(nt + ntest, p1, p2, NULL, Trgt$Î£Î¾t[1:p2, 1:p2], A, trgt = T)
  Dt <- Dtrgt[1:nt,]
  Dtest <- Dtrgt[1:ntest + nt,]
  Yt     = as.matrix(Dt) %*% Trgt$Î²t[1:p] + Îµt
  Ytest  = as.matrix(Dtest) %*% Trgt$Î²t[1:p] + Îµtest
  
  ð’¯ <- tibble(Y = c(Yt), X = Dt$X) # missing Z
  testdata = tibble(Y = c(Ytest), D = Dtest) # keep Z for oracle error
  
  return(
    list(
      proxydata = ð’«,
      targetdata = ð’¯,
      testdata = testdata,
      p1 = p1,
      p2 = p2,
      K = K,
      Î²t = Trgt$Î²t[1:p],
      Î˜t = Trgt$Î˜t[1:p1, 1:p2]
    )
  )
}


# Estimation --------------------------------------------------------------


### Method 1: Target Lasso only

lasso_trgt = function(ð’Ÿ) {
  p1 = ð’Ÿ$p1
  Yt = ð’Ÿ$targetdata$Y
  Xt = ð’Ÿ$targetdata$X
  Ytest = ð’Ÿ$testdata$Y
  Xtest = ð’Ÿ$testdata$D$X
  
  lassotarget <- cv.glmnet(Xt, Yt, alpha = 1, intercept = F)
  hÎ²1t <- coef(lassotarget, 'lambda.min')
  Ypred <- predict(lassotarget, newx = Xtest)
  pe = norm(Ytest - Ypred, '2')
  ee = norm(ð’Ÿ$Î²t[1:p1] - hÎ²1t[1:p1 + 1], '2')
  return(c(pe = pe, ee = ee))
}

### Method 3:Joint Estimator (Bastani) -  ignoring the missing covariates.. only use X

TL_matched = function(ð’Ÿ, Î´sti) {
  p1 = ð’Ÿ$p1
  hÎ²p1 <- rep(0, p1)
  for (k in 1:ð’Ÿ$K) {
    Prxyk <- ð’Ÿ$proxydata[[Î´sti]][[k]]
    Yp  = Prxyk$Y
    Xp = Prxyk$D$X
    estproxy = lm(Yp ~ Xp - 1)
    hÎ²p1 <- hÎ²p1 + coef(estproxy) / ð’Ÿ$K
  }
  Xt = ð’Ÿ$targetdata$X
  Xtest = ð’Ÿ$testdata$D$X
  Yt = ð’Ÿ$targetdata$Y
  Ytest = ð’Ÿ$testdata$Y
  
  tYt         = c(Yt - Xt %*% hÎ²p1)
  combinedlasso = cv.glmnet(Xt, tYt, alpha = 1, intercept = F)
  hdelta        = coef(combinedlasso)
  Ypred         = predict(combinedlasso, newx = Xtest) + c(Xtest %*% hÎ²p1)
  pe = norm(Ytest - Ypred, '2')
  ee = norm(ð’Ÿ$Î²t[1:p1] - hdelta[1:p1 + 1] - hÎ²p1, '2')
  return(c(pe = pe, ee = ee))
}


### Method 4 (Proposed): Two-Stage Joint Estimator - including the imputation stage which uses X as a predictor


TL_impute = function(ð’Ÿ, Î´sti) {
  p1 = ð’Ÿ$p1
  p2 = ð’Ÿ$p2
  p <- p1 + p2
  hÎ²p <- rep(0, p)
  M <- 10 * p
  hÎ˜ <- matrix(0)
  for (k in 1:ð’Ÿ$K) {
    hÎ˜k <- 0
    Prxyk <- ð’Ÿ$proxydata[[Î´sti]][[k]]
    Yp  = Prxyk$Y
    Xp = Prxyk$D$X
    Zp = Prxyk$D$Z
    for (j in 1:p2) {
      df <- data.frame(Zp[, j], Xp)
      colnames(df) <- c('Y', paste0('X', 1:p1))
      sv.cv.fit <- jae_sv_cv(df, type = 'cosine', n_folds = 5)
      hÎ˜k <- cbind_pad0(hÎ˜k, sv.cv.fit$sieve.fit$beta_hat) %>% as.matrix
    }
    hÎ˜ <- add_mats_pad(hÎ˜, hÎ˜k[, -1] / ð’Ÿ$K)
    estproxy  = lm(Yp ~ Xp + Zp - 1)
    hÎ²p <- hÎ²p + coef(estproxy) / ð’Ÿ$K
  }
  M <- nrow(hÎ˜)
  Yt = ð’Ÿ$targetdata$Y
  Xt = ð’Ÿ$targetdata$X
  Ytest = ð’Ÿ$testdata$Y
  Xtest = ð’Ÿ$testdata$D$X
  
  #### Stage 1: Imputation - using the information of X
  t.sv.prp <- sieve_preprocess(Xt, M, type = 'cosine')
  tZt = t.sv.prp$Phi %*% hÎ˜
  
  #### Stage 2: Local debiasing
  tYt = c(Yt - Xt %*% hÎ²p[1:p1] - tZt %*% hÎ²p[(p1 + 1):p])
  combinedlassonew = cv.glmnet(cbind(Xt, tZt), tYt, alpha = 1, intercept = F)
  
  # Test errors
  test.sv.prp <- sieve_preprocess(Xtest, M, type = 'cosine')
  tZtest = test.sv.prp$Phi %*% hÎ˜
  Ypred = predict(combinedlassonew, newx = cbind(Xtest, tZtest)) +
    c(Xtest %*% hÎ²p[1:p1] + tZtest %*% hÎ²p[(p1 + 1):p])
  
  hdelta = coef(combinedlassonew)
  ee = norm(abs(ð’Ÿ$Î²t[1:p] - hdelta[1:p] - hÎ²p[1:p]), '2')
  pe = norm(abs(Ytest - Ypred), '2')
  return(c(pe = pe, ee = ee))
}


# oracle ------------------------------------------------------------------


oracle = function(ð’Ÿ, Trgt) {
  p = ð’Ÿ$p1 + ð’Ÿ$p2
  Ytest = ð’Ÿ$testdata$Y
  Xtest = ð’Ÿ$testdata$D
  Ypred = as.matrix(Xtest) %*% Trgt$Î²t[1:p]
  pe = norm(Ytest - Ypred, '2')
  return(c(pe = pe, ee = 0))
}